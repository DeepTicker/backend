#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ê¸ˆìœµ ê°ì •ë¶„ì„ Flask ì„œë²„
ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë”©í•˜ê³  ìƒì£¼ì‹œì¼œ ë¹ ë¥¸ ë¶„ì„ ì œê³µ
"""

from flask import Flask, request, jsonify
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import torch
import logging
import sys
import os
import json
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

app = Flask(__name__)

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ ì €ì¥
sentiment_analyzer = None
tokenizer = None
model = None

def load_model():
    """ê°ì •ë¶„ì„ ëª¨ë¸ ë¡œë”©"""
    global sentiment_analyzer, tokenizer, model
    
    try:
        logger.info("ğŸ”„ ê°ì •ë¶„ì„ ëª¨ë¸ ë¡œë”© ì‹œì‘...")
        
        # 1ì°¨ ëª¨ë¸: í•œêµ­ ê¸ˆìœµíŠ¹í™” ëª¨ë¸
        model_name = "krevas/finance-koelectra-small-discriminator"
        
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForSequenceClassification.from_pretrained(model_name)
            sentiment_analyzer = pipeline(
                "text-classification",
                model=model,
                tokenizer=tokenizer,
                device=0 if torch.cuda.is_available() else -1  # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ì‚¬ìš©
            )
            logger.info(f"âœ… 1ì°¨ ëª¨ë¸ ë¡œë”© ì„±ê³µ: {model_name}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ 1ì°¨ ëª¨ë¸ ì‹¤íŒ¨: {e}")
            # 2ì°¨ ëª¨ë¸: ëŒ€ì²´ ëª¨ë¸
            model_name = "Copycats/koelectra-base-v3-generalized-sentiment-analysis"
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForSequenceClassification.from_pretrained(model_name)
            sentiment_analyzer = pipeline(
                "text-classification",
                model=model,
                tokenizer=tokenizer,
                device=0 if torch.cuda.is_available() else -1
            )
            logger.info(f"âœ… 2ì°¨ ëª¨ë¸ ë¡œë”© ì„±ê³µ: {model_name}")
            
        logger.info("ğŸš€ ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ì„œë²„ ì¤€ë¹„ë¨")
        
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        raise e

def analyze_sentiment_for_entity(entity_name, entity_type, content):
    """ê°œë³„ ì—”í‹°í‹°ì— ëŒ€í•œ ê°ì •ë¶„ì„"""
    try:
        # ì—”í‹°í‹°ì™€ ê´€ë ¨ëœ ë¬¸ë§¥ ì¶”ì¶œ
        sentences = content.split('.')
        relevant_sentences = []
        
        for sentence in sentences:
            if entity_name in sentence:
                relevant_sentences.append(sentence.strip())
        
        # ê´€ë ¨ ë¬¸ë§¥ì´ ì—†ìœ¼ë©´ ì „ì²´ ë‚´ìš© ì‚¬ìš©
        if not relevant_sentences:
            context = content[:200]  # ì²˜ìŒ 200ìë§Œ ì‚¬ìš©
        else:
            context = '. '.join(relevant_sentences[:2])  # ìµœëŒ€ 2ë¬¸ì¥
        
        # ë¶„ì„ í…ìŠ¤íŠ¸ êµ¬ì„±
        analysis_text = f"{entity_name}: {context}"
        
        # ê°ì •ë¶„ì„ ì‹¤í–‰
        result = sentiment_analyzer(analysis_text[:512])  # í† í° ì œí•œ
        
        # ê²°ê³¼ í•´ì„
        sentiment_label = result[0]['label'].upper()
        confidence_score = float(result[0]['score']) * 100
        
        # ë¼ë²¨ í‘œì¤€í™”
        if sentiment_label in ['POSITIVE', 'POS', '1']:
            sentiment = "+"
        elif sentiment_label in ['NEGATIVE', 'NEG', '0']:
            sentiment = "-"
        else:
            sentiment = "0"  # ì¤‘ë¦½
        
        return {
            "entity_name": entity_name,
            "entity_type": entity_type,
            "sentiment": sentiment,
            "confidence_score": round(confidence_score, 1)
            # reasoning ì œê±° - ê°œë³„ì£¼/í…Œë§ˆ/ì‚°ì—…ì—ëŠ” ë¶ˆí•„ìš”
        }
        
    except Exception as e:
        logger.error(f"ì—”í‹°í‹° '{entity_name}' ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {
            "entity_name": entity_name,
            "entity_type": entity_type,
            "sentiment": "0",
            "confidence_score": 0
            # reasoning ì œê±° - ê°œë³„ì£¼/í…Œë§ˆ/ì‚°ì—…ì—ëŠ” ë¶ˆí•„ìš”
        }

@app.route('/health', methods=['GET'])
def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return jsonify({
        "status": "healthy",
        "model_loaded": sentiment_analyzer is not None,
        "timestamp": datetime.now().isoformat()
    })

@app.route('/analyze', methods=['POST'])
def analyze_sentiment():
    """ë°°ì¹˜ ê°ì •ë¶„ì„ API"""
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({"success": False, "error": "JSON ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤"}), 400
        
        entities = data.get('entities', [])
        content = data.get('content', '')
        
        if not entities or not content:
            return jsonify({"success": False, "error": "entitiesì™€ contentê°€ í•„ìš”í•©ë‹ˆë‹¤"}), 400
        
        logger.info(f"ğŸ“Š ë°°ì¹˜ ë¶„ì„ ì‹œì‘: {len(entities)}ê°œ ì—”í‹°í‹°")
        
        results = []
        
        # ğŸ”¥ ë°°ì¹˜ ì²˜ë¦¬: ëª¨ë“  ì—”í‹°í‹°ë¥¼ í•œë²ˆì— ì²˜ë¦¬
        for entity in entities:
            entity_name = entity.get('name', '')
            entity_type = entity.get('type', 'unknown')
            
            if not entity_name:
                continue
                
            result = analyze_sentiment_for_entity(entity_name, entity_type, content)
            results.append(result)
        
        logger.info(f"âœ… ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
        
        return jsonify({
            "success": True,
            "results": results,
            "total_processed": len(results),
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"âŒ ë¶„ì„ API ì˜¤ë¥˜: {e}")
        return jsonify({
            "success": False,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }), 500

@app.route('/analyze/single', methods=['POST'])
def analyze_single():
    """ë‹¨ì¼ ì—”í‹°í‹° ê°ì •ë¶„ì„ API"""
    try:
        data = request.get_json()
        
        entity_name = data.get('entity_name', '')
        entity_type = data.get('entity_type', 'unknown')
        content = data.get('content', '')
        
        if not entity_name or not content:
            return jsonify({"success": False, "error": "entity_nameê³¼ contentê°€ í•„ìš”í•©ë‹ˆë‹¤"}), 400
        
        result = analyze_sentiment_for_entity(entity_name, entity_type, content)
        
        return jsonify({
            "success": True,
            "result": result,
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"âŒ ë‹¨ì¼ ë¶„ì„ API ì˜¤ë¥˜: {e}")
        return jsonify({
            "success": False,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }), 500

if __name__ == '__main__':
    try:
        # ëª¨ë¸ ë¡œë”©
        load_model()
        
        # ì„œë²„ ì‹œì‘
        logger.info("ğŸŒŸ Flask ê°ì •ë¶„ì„ ì„œë²„ ì‹œì‘")
        logger.info("ğŸ“ URL: http://127.0.0.1:5555")
        logger.info("ğŸ“ Health Check: http://127.0.0.1:5555/health")
        logger.info("ğŸ“ API: POST http://127.0.0.1:5555/analyze")
        
        app.run(
            host='127.0.0.1',
            port=5555,
            debug=False,  # ìš´ì˜ í™˜ê²½ì—ì„œëŠ” False
            threaded=True  # ë‹¤ì¤‘ ìš”ì²­ ì²˜ë¦¬
        )
        
    except Exception as e:
        logger.error(f"ğŸ’¥ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
        sys.exit(1) 